{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk import WhitespaceTokenizer\n",
    "from nltk.corpus import stopwords, words, wordnet\n",
    "from nltk.lm import Vocabulary\n",
    "from collections import OrderedDict\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import concurrent.futures\n",
    "from torch.optim import Adam\n",
    "from data_loading import process_text_df, NewsText, tensorize_sentences, collate_fn\n",
    "from tqdm import tqdm as pbar\n",
    "from models import NewsNet\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import tensor\n",
    "import matplotlib.pyplot as plt\n",
    "import tqdm\n",
    "stopwords = stopwords.words()\n",
    "words = words.words() \n",
    "wordnet = wordnet.words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk import WhitespaceTokenizer\n",
    "from nltk.corpus import stopwords, words, wordnet\n",
    "from nltk.lm import Vocabulary\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import concurrent.futures\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from nltk.stem.snowball import EnglishStemmer\n",
    "from torch import tensor\n",
    "stopwords = stopwords.words()\n",
    "words = words.words() \n",
    "wordnet = wordnet.words()\n",
    "\n",
    "class process_text_df():\n",
    "    \n",
    "    def __init__(self, df, text_cols):\n",
    "        self.df = df.copy()\n",
    "        self.text_cols = text_cols\n",
    "        self.stemmer = EnglishStemmer()\n",
    "        \n",
    "    def word_only(self, l):\n",
    "        nopunkt = lambda w: ''.join([char for char in w if char.isalnum()])\n",
    "        l = [nopunkt(w) for w in l]\n",
    "        return l\n",
    "\n",
    "    def clean_text_col(self, text_col):\n",
    "        text_col = text_col.apply(lambda text: WhitespaceTokenizer().tokenize(text))\n",
    "        text_col = text_col.apply(lambda sent: [word.lower() for word in sent])\n",
    "        text_col = text_col.apply(lambda sent: [word for word in sent if word not in stopwords])\n",
    "        text_col = text_col.apply(lambda sent: self.word_only(sent))\n",
    "        text_col = text_col.apply(lambda sent: [self.stemmer.stem(word) for word in sent])\n",
    "        return text_col\n",
    "\n",
    "    def chunk_arr(self, arr, n_partitions=8):\n",
    "        size = len(arr) // n_partitions\n",
    "        out = [arr[i * size:(i + 1) * size] for i in range(n_partitions + 1)]\n",
    "        return out\n",
    "\n",
    "    def clean_tokenize(self, text_col):\n",
    "        with concurrent.futures.ProcessPoolExecutor(4) as executor:\n",
    "            chunks = self.chunk_arr(self.df[text_col], 4)\n",
    "            results = executor.map(self.clean_text_col, chunks)\n",
    "            out = [result for result in results]\n",
    "        out = pd.concat(out)\n",
    "        return out\n",
    "\n",
    "    def process_text_col(self):\n",
    "        for text_col in self.text_cols:\n",
    "            self.df[text_col] = self.clean_tokenize(text_col)\n",
    "            \n",
    "    def build_vocab(self):\n",
    "        out = []\n",
    "        for col in self.text_cols:\n",
    "            col_ = self.df[col]\n",
    "            extend = [w for sent in col_ for w in sent]\n",
    "            out.extend(extend)\n",
    "        out = list(Vocabulary(out, unk_cutoff=100))\n",
    "        out = {out[i]:len(out) - (i + 1) for i in range(len(out))}\n",
    "        self.vocab = out\n",
    "    \n",
    "    def tokenize_sentences(self):\n",
    "        self.build_vocab()\n",
    "        for text_col in self.text_cols:\n",
    "            self.df[text_col] =\\\n",
    "            self.df[text_col].apply(lambda sent: [word if word in self.vocab else '<UNK>' for word in sent])\n",
    "            self.df[text_col] =\\\n",
    "            self.df[text_col].apply(lambda sent: [self.vocab[word] for word in sent])\n",
    "        \n",
    "def tensorize_sentences(text_series, labels):\n",
    "    sentences, labels = [torch.tensor(text) for text in text_series], \\\n",
    "                        tensor(labels.apply(lambda l: 1 if l == 'true' else 0))\n",
    "    return sentences, labels\n",
    "\n",
    "class NewsText(Dataset):\n",
    "\n",
    "    def __init__(self, news_text_list, labels):\n",
    "        self.news_text_list = news_text_list\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        assert(len(self.news_text_list) == len(self.labels))\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.news_text_list[idx], self.labels[idx]\n",
    "        return sample\n",
    "\n",
    "def pad_sent(sents, max_seq_len):\n",
    "    max_seq_len = min(100, max_seq_len)\n",
    "    out = []\n",
    "    for i in range(len(sents)):\n",
    "        sent = sents[i]\n",
    "        append_tensor = tensor([sent[j] if j < len(sent) else 0 for j in range(max_seq_len)]).unsqueeze(0)\n",
    "        out.append(append_tensor)\n",
    "    out = torch.cat(out)\n",
    "    return out\n",
    "    \n",
    "\n",
    "def collate_fn(sample):\n",
    "\n",
    "    labels = tensor([s[1] for s in sample])\n",
    "    sents = [s[0] for s in sample]\n",
    "    max_seq_len = max([sent.shape[0] for sent in sents])\n",
    "    sents = pad_sent(sents, max_seq_len)\n",
    "    return sents, labels\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake, true = pd.read_csv('Fake.csv'), pd.read_csv('True.csv')\n",
    "fake['label'] = 'fake'\n",
    "true['label'] = 'true'\n",
    "news = pd.concat((fake, true))\n",
    "news = news.sample(frac=1)\n",
    "news.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "p1 = process_text_df(news, ['title', 'text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<timed eval>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-7fc055bfdf82>\u001b[0m in \u001b[0;36mprocess_text_col\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mprocess_text_col\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtext_col\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext_cols\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtext_col\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclean_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_col\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mbuild_vocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-7fc055bfdf82>\u001b[0m in \u001b[0;36mclean_tokenize\u001b[0;34m(self, text_col)\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0mchunks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchunk_arr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtext_col\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m             \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexecutor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclean_text_col\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mresult\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-7fc055bfdf82>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0mchunks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchunk_arr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtext_col\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m             \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexecutor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclean_text_col\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mresult\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/news/lib/python3.6/concurrent/futures/process.py\u001b[0m in \u001b[0;36m_chain_from_iterable_of_lists\u001b[0;34m(iterable)\u001b[0m\n\u001b[1;32m    364\u001b[0m     \u001b[0mcareful\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mto\u001b[0m \u001b[0mkeep\u001b[0m \u001b[0mreferences\u001b[0m \u001b[0mto\u001b[0m \u001b[0myielded\u001b[0m \u001b[0mobjects\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m     \"\"\"\n\u001b[0;32m--> 366\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0melement\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    367\u001b[0m         \u001b[0melement\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreverse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    368\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0melement\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/news/lib/python3.6/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult_iterator\u001b[0;34m()\u001b[0m\n\u001b[1;32m    584\u001b[0m                     \u001b[0;31m# Careful not to keep a reference to the popped future\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m                         \u001b[0;32myield\u001b[0m \u001b[0mfs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m                         \u001b[0;32myield\u001b[0m \u001b[0mfs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mend_time\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmonotonic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/news/lib/python3.6/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    425\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    426\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 427\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_condition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    428\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mCANCELLED\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCANCELLED_AND_NOTIFIED\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/news/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    293\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "p1.process_text_col()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>subject</th>\n",
       "      <th>date</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[mladic, verdict, carri, messag, syria, beyond...</td>\n",
       "      <td>GENEVA (Reuters) - The conviction of former Bo...</td>\n",
       "      <td>worldnews</td>\n",
       "      <td>November 22, 2017</td>\n",
       "      <td>true</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[watch, chuck, schumer, fake, cri, muslim, pol...</td>\n",
       "      <td>Chuck Schumer s crying over Muslim refugees.Af...</td>\n",
       "      <td>politics</td>\n",
       "      <td>Jan 29, 2017</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[muslim, invas, updat, gang, member, germani, ...</td>\n",
       "      <td>Deep down, they just want to assimilate Eight ...</td>\n",
       "      <td>politics</td>\n",
       "      <td>Oct 21, 2015</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[german, court, rule, favor, third, gender, ca...</td>\n",
       "      <td>BERLIN (Reuters) - Germany s highest court rul...</td>\n",
       "      <td>worldnews</td>\n",
       "      <td>November 8, 2017</td>\n",
       "      <td>true</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[russian, foreign, ministri, meet, visit, nort...</td>\n",
       "      <td>MOSCOW (Reuters) - Russia s foreign ministry p...</td>\n",
       "      <td>worldnews</td>\n",
       "      <td>September 26, 2017</td>\n",
       "      <td>true</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44893</th>\n",
       "      <td>[you, donald, trump, worst, nightmar, john, ke...</td>\n",
       "      <td>U.S. Secretary of State John Kerry delivered a...</td>\n",
       "      <td>News</td>\n",
       "      <td>May 8, 2016</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44894</th>\n",
       "      <td>[germani, antitrump, open, border, angela, mer...</td>\n",
       "      <td>In what has to be considered an historic about...</td>\n",
       "      <td>left-news</td>\n",
       "      <td>Nov 28, 2016</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44895</th>\n",
       "      <td>[watch, new, documentari, explor, trump, russi...</td>\n",
       "      <td>This past week the world has been hit with man...</td>\n",
       "      <td>News</td>\n",
       "      <td>May 16, 2017</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44896</th>\n",
       "      <td>[turkey, say, us, isol, jerusalem, issu, threat]</td>\n",
       "      <td>ANKARA (Reuters) - Turkey said on Wednesday th...</td>\n",
       "      <td>worldnews</td>\n",
       "      <td>December 20, 2017</td>\n",
       "      <td>true</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44897</th>\n",
       "      <td>[canada, stall, mali, mission, could, hit, sec...</td>\n",
       "      <td>ST. JOHN S, Newfoundland (Reuters) - Canada on...</td>\n",
       "      <td>worldnews</td>\n",
       "      <td>September 13, 2017</td>\n",
       "      <td>true</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>44898 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   title  \\\n",
       "0      [mladic, verdict, carri, messag, syria, beyond...   \n",
       "1      [watch, chuck, schumer, fake, cri, muslim, pol...   \n",
       "2      [muslim, invas, updat, gang, member, germani, ...   \n",
       "3      [german, court, rule, favor, third, gender, ca...   \n",
       "4      [russian, foreign, ministri, meet, visit, nort...   \n",
       "...                                                  ...   \n",
       "44893  [you, donald, trump, worst, nightmar, john, ke...   \n",
       "44894  [germani, antitrump, open, border, angela, mer...   \n",
       "44895  [watch, new, documentari, explor, trump, russi...   \n",
       "44896   [turkey, say, us, isol, jerusalem, issu, threat]   \n",
       "44897  [canada, stall, mali, mission, could, hit, sec...   \n",
       "\n",
       "                                                    text    subject  \\\n",
       "0      GENEVA (Reuters) - The conviction of former Bo...  worldnews   \n",
       "1      Chuck Schumer s crying over Muslim refugees.Af...   politics   \n",
       "2      Deep down, they just want to assimilate Eight ...   politics   \n",
       "3      BERLIN (Reuters) - Germany s highest court rul...  worldnews   \n",
       "4      MOSCOW (Reuters) - Russia s foreign ministry p...  worldnews   \n",
       "...                                                  ...        ...   \n",
       "44893  U.S. Secretary of State John Kerry delivered a...       News   \n",
       "44894  In what has to be considered an historic about...  left-news   \n",
       "44895  This past week the world has been hit with man...       News   \n",
       "44896  ANKARA (Reuters) - Turkey said on Wednesday th...  worldnews   \n",
       "44897  ST. JOHN S, Newfoundland (Reuters) - Canada on...  worldnews   \n",
       "\n",
       "                      date label  \n",
       "0       November 22, 2017   true  \n",
       "1             Jan 29, 2017  fake  \n",
       "2             Oct 21, 2015  fake  \n",
       "3        November 8, 2017   true  \n",
       "4      September 26, 2017   true  \n",
       "...                    ...   ...  \n",
       "44893          May 8, 2016  fake  \n",
       "44894         Nov 28, 2016  fake  \n",
       "44895         May 16, 2017  fake  \n",
       "44896   December 20, 2017   true  \n",
       "44897  September 13, 2017   true  \n",
       "\n",
       "[44898 rows x 5 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p1.df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p1.tokenize_sentences()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l, labs = tensorize_sentences(p1.df.text.apply(lambda sent: sent[:50]), p1.df.label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# l, labs = tensorize_sentences(p1.df.text.apply(lambda sent: sent[:100]), p1.df.label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "net = NewsNet(p1.vocab, hidden_size=4, embedding_dim=8, num_layers=2)\n",
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "optimizer = Adam(net.parameters(), lr=1e-4, weight_decay=5e-6)\n",
    "idx = len(l) // 4\n",
    "\n",
    "l_tr, labs_tr = l[:-2 * idx], labs[: -2 * idx]\n",
    "l_val, labs_val = l[-2 * idx:-idx], labs[-2 * idx:-idx]\n",
    "l_tst, labs_tst = l[-idx:], labs[-idx:]\n",
    "l_tr, l_val, labs_tr, labs_val = l[:idx], l[idx:], labs[:idx], labs[idx:]\n",
    "tr_set = NewsText(l_tr, labs_tr)\n",
    "val_set = NewsText(l_val, labs_val)\n",
    "_, val_set = enumerate(DataLoader(val_set, batch_size=len(val_set), collate_fn=collate_fn)).__next__()\n",
    "val_features, val_labels = val_set\n",
    "val_labels = val_labels.unsqueeze(-1).float()\n",
    "loader = DataLoader(tr_set, batch_size=4, collate_fn=collate_fn)\n",
    "loss_list = []\n",
    "val_loss_list = []\n",
    "accuracy_list = []\n",
    "epochs = 50\n",
    "for i in range(epochs):\n",
    "    print(f'Epoch {i + 1}')\n",
    "    for sents, labels in pbar(loader):\n",
    "        net.train()\n",
    "        labels = labels.float().unsqueeze(-1)\n",
    "        out = net(sents)\n",
    "        loss = loss_fn(out, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        net.eval()\n",
    "    with torch.no_grad():\n",
    "        val_out = net(val_features)\n",
    "        val_loss = loss_fn(val_out, val_labels)\n",
    "        val_loss_list.append(val_loss.item())\n",
    "        val_guesses = torch.round(nn.Sigmoid()(val_out))\n",
    "        accuracy = (val_guesses == val_labels).float().mean().item()\n",
    "        \n",
    "        accuracy_list.append(accuracy)\n",
    "        \n",
    "    loss_list.append(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "pd.Series(loss_list).plot(ax=ax, label='Tr')\n",
    "pd.Series(val_loss_list).plot(ax=ax, label='Val')\n",
    "fig.set_size_inches(fig.get_size_inches() * 1.5)\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(accuracy_list).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tst_set = NewsText(l_tst, labs_tst)\n",
    "_, tst_set = enumerate(DataLoader(tst_set, batch_size=len(tst_set), collate_fn=collate_fn)).__next__()\n",
    "tst_features, tst_labels = tst_set\n",
    "tst_labels = tst_labels.unsqueeze(-1).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    tst_out = net(tst_features)\n",
    "    tst_loss = loss_fn(tst_out, tst_labels)\n",
    "    tst_guesses = torch.round(nn.Sigmoid()(tst_out))\n",
    "    accuracy_tst = (tst_guesses == tst_labels).float().mean().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_tst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pretrained_embeddings = net.word_embeddings\n",
    "net2 = NewsNet(p1.vocab, hidden_size=4, embedding_dim=8, num_layers=2, pretrained_embeddings=pretrained_embeddings)\n",
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "optimizer = Adam(net2.parameters(), lr=1e-4, weight_decay=5e-6)\n",
    "idx = len(l) // 4\n",
    "\n",
    "l_tr, labs_tr = l[:-2 * idx], labs[: -2 * idx]\n",
    "l_val, labs_val = l[-2 * idx:-idx], labs[-2 * idx:-idx]\n",
    "l_tst, labs_tst = l[-idx:], labs[-idx:]\n",
    "l_tr, l_val, labs_tr, labs_val = l[:idx], l[idx:], labs[:idx], labs[idx:]\n",
    "tr_set = NewsText(l_tr, labs_tr)\n",
    "val_set = NewsText(l_val, labs_val)\n",
    "_, val_set = enumerate(DataLoader(val_set, batch_size=len(val_set), collate_fn=collate_fn)).__next__()\n",
    "val_features, val_labels = val_set\n",
    "val_labels = val_labels.unsqueeze(-1).float()\n",
    "loader = DataLoader(tr_set, batch_size=4, collate_fn=collate_fn)\n",
    "loss_list = []\n",
    "val_loss_list = []\n",
    "accuracy_list = []\n",
    "epochs = 40\n",
    "for i in range(epochs):\n",
    "    print(f'Epoch {i + 1}')\n",
    "    for sents, labels in pbar(loader):\n",
    "        net2.train()\n",
    "        labels = labels.float().unsqueeze(-1)\n",
    "        out = net2(sents)\n",
    "        loss = loss_fn(out, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        net2.eval()\n",
    "    with torch.no_grad():\n",
    "        val_out = net2(val_features)\n",
    "        val_loss = loss_fn(val_out, val_labels)\n",
    "        val_loss_list.append(val_loss.item())\n",
    "        val_guesses = torch.round(nn.Sigmoid()(val_out))\n",
    "        accuracy = (val_guesses == val_labels).float().mean().item()\n",
    "        \n",
    "        accuracy_list.append(accuracy)\n",
    "        \n",
    "    loss_list.append(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "pd.Series(loss_list).plot(ax=ax, label='Tr')\n",
    "pd.Series(val_loss_list).plot(ax=ax, label='Val')\n",
    "fig.set_size_inches(fig.get_size_inches() * 1.5)\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(accuracy_list).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loss_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tst_set = NewsText(l_tst, labs_tst)\n",
    "_, tst_set = enumerate(DataLoader(tst_set, batch_size=len(tst_set), collate_fn=collate_fn)).__next__()\n",
    "tst_features, tst_labels = tst_set\n",
    "tst_labels = tst_labels.unsqueeze(-1).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    tst_out = net2(tst_features)\n",
    "    tst_loss = loss_fn(tst_out, tst_labels)\n",
    "    tst_guesses = torch.round(nn.Sigmoid()(tst_out))\n",
    "    accuracy_tst = (tst_guesses == tst_labels).float().mean().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_tst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "news",
   "language": "python",
   "name": "news"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
